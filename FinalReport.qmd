---
title: "Cardiovascular Disease Report"
authors: "Valerio Martinez, Cruz Salas, Ryan Nguyen, Simrat Clair, Alan Johnson"
date: 2024-04-28
format: pdf
highlight-style: github
code-block-bg: true
code-block-border-left: "#31BAE9"
title-block-style: plain
---

\newpage

# Introduction

Our project involves analyzing the Cardiovascular Disease dataset found on Kaggle to determine which factors significantly contribute to the development of cardiovascular disease in an individual. We were inspired by lecture 9, which introduced logistic regression using the BreastCancer dataset to analyze attributes of cells that imply whether it is benign or malignant. The dataset contains exactly 70,000 observations, in which each observation is associated with 13 variables.

**Input Descriptions:**

-   age: Shown in days.
-   height: Shown in cm.
-   weight: Shown in float kg.
-   gender: Shown as a categorical code.
-   ap_hi: Systolic blood pressure.
-   ap_lo: Diastolic blood pressure.
-   cholesterol: Shown in three different ways: 1: normal cholesterol, 2: above normal cholesterol, and 3: well above cholesterol.
-   gluc: Shown in three different ways: gluc1: normal glucose, gluc2: above normal glucose, and gluc3: well above normal glucose.
-   smoke: Binary classification variable: smoke0: non-smoker, smoke1: smoker.
-   alco: Binary classification variable: alco0: low alcohol intake, alco1: high alcohol intake.
-   active: Binary classification variable: active0: low physical activity, active1: regular physical activity.

**Output Description:**

-   cardio: Indicates the presence or absence of cardiovascular disease.

**Research Question**

How do lifestyle factors correlate with the risk of developing cardiovascular disease?

# Methods

## Logistic Model (Ryan Nguyen, Alan Johnson)

We chose a logistic model because our response variable, cardio, is categorial (0 or 1). Also, the logistic model can handle large datasets efficiently, making it suitable for our dataset which contains 70,000 observations.

Some disadvantages of the logistic model is that it can be sensitive to outliers, which may disproportionately influence the model's coefficients and predictions. Also, if the relationship between predictors and the log-odds of the outcome is highly non-linear or complex, logistic model may not capture it effectively.

### Model Formula

```{=tex}
\begin{align}
P(\text{cardio}=1|X) = \frac{%
\begin{aligned}[t]
& \exp(\beta_0 + \beta_1 \times \text{age}+ \beta_2 \times \text{height}  + \beta_3 \times \text{weight} + \beta_4 \times \text{gender} + \beta_5 \times \text{ap\_hi} \\
& + \beta_6 \times \text{ap\_lo} + \beta_7 \times \text{cholesterol} + \beta_8 \times \text{gluc} + \beta_9 \times \text{smoke} + \beta_{10} \times \text{alco}\\
& + \beta_{11} \times \text{active})
\end{aligned}
}{%
\begin{aligned}[t]
& 1 + \exp(\beta_0 + \beta_1 \times \text{age}+ \beta_2 \times \text{height}  + \beta_3 \times \text{weight} + \beta_4 \times \text{gender} + \beta_5 \times \text{ap\_hi} \\
& + \beta_6 \times \text{ap\_lo} + \beta_7 \times \text{cholesterol} + \beta_8 \times \text{gluc} + \beta_9 \times \text{smoke} + \beta_{10} \times \text{alco}\\
& + \beta_{11} \times \text{active})
\end{aligned}
}
\end{align}
```
**Model 1 - Include all predictors**

```{r, warning = FALSE, output = FALSE}
library(readr)
cardio.data <- read_delim("cardio_train.csv",
                           delim = ";", escape_double = FALSE, trim_ws = TRUE)

cardio.data$gender = as.factor(cardio.data$gender)
cardio.data$cholesterol = as.factor(cardio.data$cholesterol)
cardio.data$gluc = as.factor(cardio.data$gluc)
cardio.data$smoke = as.factor(cardio.data$smoke)
cardio.data$alco = as.factor(cardio.data$alco)
cardio.data$active = as.factor(cardio.data$active)
cardio.data$cardio = as.factor(cardio.data$cardio)

heart.logistic1 =  glm(cardio  ~  . - id,  family  =  "binomial",
                      data  =  cardio.data)

```
```{r}
summary(heart.logistic1)

```

Predictors with p-values less than 0.05 are considered significant. For example, age, height, weight, blood pressure, cholesterol levels, smoking status, alcohol consumption, and physical activity are all significant predictors. Gender, glucose level don't appear to be significant predictors as their p-values exceed chosen significance level.

**Model 2 - Only include statistically significant predictors**

We will use the significant predictors found from heart.logistic1 to to create Model 2. This means all predictors except for id, gender, and glucose will be included.

```{r, warning = FALSE, output = FALSE}
heart.logistic2 =  glm(cardio  ~  age+height+weight+ap_hi+ap_lo+cholesterol+smoke+alco+active
                         ,  family  =  "binomial",
                      data  =  cardio.data)
```

We next applied the step() function on the initial model (heart.logistic1) to perform stepwise regression based on the Akaike Information Criterion (AIC). The stepwise process sequentially evaluates each predictor's contribution to the model and removes predictors that do not significantly improve the model fit, based on AIC.

```{r,  warning=FALSE}
step(heart.logistic1)
```

**Model 3 - Using predictors from stepwise regression**

After the stepwise regression process, the final selected predictors were age, height, weight, ap_hi (systolic blood pressure), ap_lo (diastolic blood pressure), cholesterol, gluc, smoke, alco (alcohol consumption), and active (physical activity). The coefficients for these predictors are provided in the summary output of the final model (heart.logistic3).

The coefficients in the final model represent the log odds of the outcome (cardiovascular disease) associated with each unit change in the predictor, assuming other predictors are constant. For example, a positive coefficient for a predictor indicates an increase in the probability of cardiovascular disease with an increase in that predictor, while a negative coefficient indicates a decrease in the probability. 

```{r, warning = FALSE, output = FALSE}
heart.logistic3 = glm(formula = cardio ~ age + height + weight + ap_hi + ap_lo + 
    cholesterol + gluc + smoke + alco + active, family = "binomial", 
    data = cardio.data)
```

**Determining Best Model**

```{r, warning = FALSE, echo = FALSE}
extract_info  = function(model) {
  deviance <- summary(model)$null.deviance
  residual_deviance <- summary(model)$deviance
  r_squared <- 1 - (residual_deviance / deviance)
  AIC <- AIC(model)
  BIC <- BIC(model)
  
  return(c(Null_Deviance = deviance,
           Residual_Deviance = residual_deviance,
           R_Squared = r_squared,
           AIC = AIC,
           BIC = BIC))
}

# Extract information from each model
info1 <- extract_info(heart.logistic1)
info2 <- extract_info(heart.logistic2)
info3 <- extract_info(heart.logistic3)

# Create a data frame to store the information
model_info <- data.frame(
  Model = c("heart.logistic1", "heart.logistic2", "heart.logistic3"),
  Null_Deviance = c(info1["Null_Deviance"], info2["Null_Deviance"], info3["Null_Deviance"]),
  Residual_Deviance = c(info1["Residual_Deviance"], info2["Residual_Deviance"], info3["Residual_Deviance"]),
  R_Squared = c(info1["R_Squared"], info2["R_Squared"], info3["R_Squared"]),
  AIC = c(info1["AIC"], info2["AIC"], info3["AIC"]),
  BIC = c(info1["BIC"], info2["BIC"], info3["BIC"])
)
(model_info)
```

It is evident that \textcolor{red}{Model 3} is the best model out of all the models because the AIC and BIC is the lowest. The AIC of Model 3 is 80907.87 while the AIC of Model 1 is 80910.62 and the AIC of Model 2 is 81005.71. We will further test Model 3 (Best Model) on training and validation tests.

### Final Equation for Logistic Model

```{=tex}
\begin{align}
P(\text{cardio}=1|X) = \frac{%
\begin{aligned}[t]
& \exp(-8.188 + 0.0001485 \times \text{age}+ -0.005255 \times \text{height}  + 0.01521 \times \text{weight} \\
& + 0.03953\times \text{ap\_hi} + 0.0003007\times \text{ap\_lo} + 0.4218 \times \text{cholesterol2} \\
& + 1.134 \times \text{cholesterol3} +  0.03005 \times \text{gluc2} +  -0.3389 \times \text{gluc3} \\
& + -0.1256\times \text{smoke1} + -0.1681 \times \text{alco1} + -0.2101 \times \text{active1})
\end{aligned}
}{%
\begin{aligned}[t]
& 1+ \exp(-8.188 + 0.0001485 \times \text{age}+ -0.005255 \times \text{height}  + 0.01521 \times \text{weight} \\
& + 0.03953\times \text{ap\_hi} + 0.0003007\times \text{ap\_lo} + 0.4218 \times \text{cholesterol2} \\
& + 1.134 \times \text{cholesterol3} +  0.03005 \times \text{gluc2} +  -0.3389 \times \text{gluc3} \\
& + -0.1256\times \text{smoke1} + -0.1681 \times \text{alco1} + -0.2101 \times \text{active1})
\end{aligned}
}
\end{align}
```
### Training/ Validation

```{r, warning = FALSE, echo = FALSE, output = FALSE}
set.seed(100)
test_errors = numeric(10)
for(i in 1:10){
  # initialize vector to store prediction errors
sample= sample.int(n = nrow(cardio.data), size = floor(0.80*nrow(cardio.data)))
train.heart.logistic = cardio.data[sample,]
test.heart.logistic = cardio.data[-sample,]

train.logistic = glm(formula = cardio ~ age + height + weight + ap_hi + ap_lo + 
    cholesterol + gluc + smoke + alco + active, family = "binomial", 
    data = cardio.data)

glm.pred = predict.glm(train.logistic, newdata = test.heart.logistic, type = "response")

pred = predict(train.logistic, type = "response", newdata = test.heart.logistic)
val = ifelse(pred <0.5,"0", "1")
tab = table(val, test.heart.logistic$cardio)
test_errors[i] = (tab[2]+tab[3])/(tab[1]+tab[2]+tab[3]+tab[4])
}
(mean_test_error = mean(test_errors))
```

The dataset is randomly split into training and validation sets using an 80-20 split. The logistic model uses predictors from heart.logistic3 which are age, height, weight, blood pressure (ap_hi and ap_lo), cholesterol level, glucose level, smoking status, alcohol consumption, and physical activity.

The trained model is then used to predict the probability of cardiovascular disease for the validation set using the predict.glm() function.

Predicted probabilities are converted to binary predictions using a threshold of 0.5, where probabilities greater than 0.5 are classified as 1 (indicating presence of cardiovascular disease) and probabilities less than or equal to 0.5 are classified as 0 (indicating absence of cardiovascular disease).

The test error rate is calculated by creating a confusion matrix and dividing the sum of the missclassified observations (false positives and false negatives) by all the observations. Mean Test Error Rate Calculation: The mean error rate is calculated by averaging the test error rates obtained from 10 iterations of the validation process. In each iteration, the logistic model is trained on a randomly selected 80% of the data, and the mean error rate is calculated based on predictions made on the remaining 20% of the data. The test error rate provides an estimate of the model's performance in predicting cardiovascular disease risk.

The mean error rate obtained from the validation process is approximately 27.74%. This indicates that, on average, the logistic model misclassifies cardiovascular disease status in the validation set for 27.74% of observations.

### Results

```{r}
summary(heart.logistic3)
```

The inclusion of statistically significant predictors, such as age, blood pressure, cholesterol levels, smoking status, alcohol consumption, and physical activity, in the final models underscores their crucial roles in assessing cardiovascular risk. These findings align with established medical knowledge regarding the impact of these factors on heart health. Additionally, the error rate obtained from the validation process indicates that the logistic models are performing well in predicting cardiovascular disease status, with an average error rate of only around 27.74%.

Overall, the results suggest that the logistic model, when incorporating significant predictors, can effectively predict cardiovascular disease risk. By leveraging key demographic, clinical, and lifestyle factors, healthcare practitioners can utilize these models to identify individuals at higher risk of cardiovascular disease and tailor preventive strategies and interventions accordingly. These findings contribute to a better understanding of cardiovascular risk assessment and highlight the potential of logistic models in supporting proactive heart health management strategies.
\newpage

## Neural Network Model (Valerio Martinez, Simrat Clair, Cruz Salas)

We chose a Neural network model as we wanted to see if the dataset contained more complex relationships that are not linear, as well as investigating all possible interactions between the input variables having an effect on the response variable.

Advantages of using a neural network model is that it can handle complex data that is not linear and adapt to changing input, also neural networks can detect all possible interactions between predictor variables, and it can also be developed using multiple different training algorithms.

While some of the disadvantages can be the "black-box" nature and have limited ability to identify possible casual relationships, and also limited to the computational power that we have access to, as having additional data or adding more complexity could affect the time to compute a model.

### Neural Network Sketch

![Neural Network Concept](nn_sketch.jpg){width="7in"}

```{r, echo = FALSE, output = FALSE}
# Imported libraries.
library(readr)
library(NeuralNetTools)
library(nnet)

set.seed(35)

# Import the data from the csv in the correct format.
cardio.data <- read_delim("cardio_train.csv", delim = ";",
                           escape_double = FALSE, trim_ws = TRUE)

# Take all of the classification variables as their factors.
cardio.data$gender = as.factor(cardio.data$gender)
cardio.data$cholesterol = as.factor(cardio.data$cholesterol)
cardio.data$gluc = as.factor(cardio.data$gluc)
cardio.data$smoke = as.factor(cardio.data$smoke)
cardio.data$alco = as.factor(cardio.data$alco)
cardio.data$active = as.factor(cardio.data$active)
cardio.data$cardio = as.factor(cardio.data$cardio)
```

### Thought Process

We decided to use a single hidden layer as our data did not deviate that much from a linear relationship and did not have as much noise thus the data not being as complex, we decided a single hidden layer would be optimal.

We decided that using 6 nodes in the hidden layer would be a good starting point for our model as it was half of our initial input nodes which was 12. However, we found out it's essential to validate this choice through testing and adjust based on the specific needs and outcomes of your model's performance on validation datasets and could be implemented in further iterations of the model.

Regarding the nnet() function in R, "entropy" refers to the cross-entropy loss function, not an activation function. Cross-entropy is used to optimize classification accuracy by penalizing the difference between predicted probabilities and actual class labels. It complements the logistic and softmax activation functions used in nnet() for binary and multi-class classification, respectively. Thus, the term "entropy" describes the loss function used for training the model efficiently.

The initial weights in a neural network play a crucial part in the learning process, as it affect the speed of convergence in a neural network. The reason why weights can't generally be set to 0 is because the neurons in the network would receive the same signal and, therefore, will create inefficiency in training. This is why we decided to use 0.5 to make sure that the neurons are small enough to ensure that there's no numerical instability---allowing the learning to be normal, rather than too slow or diverging.

### Training/ Validation

```{r}
# Separate the data into 80% training and 20% testing.
sample = sample(1 : nrow(cardio.data), floor(nrow(cardio.data) * 0.8))
cardio.train = cardio.data[sample, ]
cardio.test = cardio.data[-sample, ]

# Build the neural network.
cardio.model = nnet(cardio ~ . - id - gender, data = cardio.train,
             size = 6, rang = 0.5, decay = 5e-2, maxit = 5000)

# Print the model's information, plot, and summary
print(cardio.model)
par(mfrow = c(1, 1), mar = c(1, 1, 1, 1))
plotnet(cardio.model)
```
### Results
```{r}
garson(cardio.model)
```

The Garson plot is used to interpret the relative importance of input features in a neural network. It decomposes the neural network weights into contributions by each input variable toward the output. In the Garson plot that we produced, it can be inferred that the variables **cholesterol, gluc, and smoke** were the variables that had the most significant factors in the cardio dataset, as these factors had the strongest correlation to increased risk of heart disease.

By analyzing the weights and influence of different inputs in the network, you can identify which factors are most predictive of cardiovascular diseases. This insight can help in understanding risk factors better and potentially guiding preventive measures as people who have significant predictors as lifestyle factors can be aware of the at-risk factors. The neural network model, trained on various patient health metrics, can predict the presence of cardiovascular disease with a reasonable level of accuracy. The model identifies key predictors and their influence, offering insights into the underlying patterns and risk factors associated with cardiovascular conditions.
\newpage

```{r, echo = FALSE}
# Training prediction values.
cardio.train.predict = predict(cardio.model, cardio.train, type = "class")
cardio.train.values = cardio.train$cardio

# Testing prediction values.
cardio.test.predict = predict(cardio.model, cardio.test, type = "class")
cardio.test.values = cardio.test$cardio

# Training confusion matrix.
# cardio.train.table = table(cardio.train.values, cardio.train.predict)
# cardio.train.table

# Testing confusion matrix.
cardio.test.table = table(cardio.test.values, cardio.test.predict)
cardio.test.table
```

The Neural Network had a testing error rate of 26.38% as seen by the confusion matrix. The model helped answer our research question by indicating factors such as cholesterol, gluc, and smoke increase the rizk of developing cardiovascular disease.

# Conclusion

The neural network model had a slightly lower test error rate than the logistic model with a 26.38% testing error rate compared to a 27.74% testing error rate of the logistic model. This shows that the neural network model performed better on the testing data set than the logistic model. The neural network model also better indicated which predictors are more statistically significant in causing heart disease, which answers our research question.

We can potentially improve the logistic model by improving our method of filtering variables to identify more statistically significant predictors, as stepwise regression eliminated only one variable. Some methods to improve the accuracy of the neural network model include training the model on more testing Data, and increasing model complexity (more hidden layers).

# Bibliography

-   https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset
-   "An Introduction to Statistical Learning with Applications in R" by Gareth M. James, Daniela Witten, Trevor Hastie, Robert Tibshirani
