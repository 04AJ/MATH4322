---
title: "MATH 4322 Final Project Group 9"
format: pdf
highlight-style: github
code-block-bg: true
code-block-border-left: "#31BAE9"
title-block-style: plain
---

## Introduction

## Logistic Regression (Ryan Nguyen, Alan Johnson)

Advantages: Interpretability: Logistic regression coefficients represent the log of the odds ratio, making it easier to interpret the impact of each predictor variable on the probability of the outcome.
Efficiency: Logistic regression can handle large datasets efficiently, making it suitable for real-time predictions.
Low Variance: It tends to perform well with small datasets and is less prone to overfitting compared to more complex models.
Assumption of Independence: Logistic regression doesn't require the predictors to be independent of each other, unlike some other models like Naive Bayes.

Disadvantages: Assumption of Linearity: Logistic regression assumes a linear relationship between the independent variables and the logit of the outcome variable. If this assumption is violated, the model's performance may suffer.
Limited Outcome: It's primarily designed for binary classification tasks and may not perform well with multi-class classification without modifications. Sensitivity to Outliers: Logistic regression can be sensitive to outliers, which may disproportionately influence the model's coefficients and predictions.
Not Suitable for Complex Relationships: If the relationship between predictors and the log-odds of the outcome is highly non-linear or complex, logistic regression may not capture it effectively.

### Model Formula




\begin{align}
P(\text{cardio}=1|X) = \frac{%
\begin{aligned}[t]
& \exp(\beta_0 + \beta_1 \times \text{age}+ \beta_2 \times \text{height}  + \beta_3 \times \text{weight} + \beta_4 \times \text{gender} + \beta_5 \times \text{ap\_hi} \\
& + \beta_6 \times \text{ap\_lo} + \beta_7 \times \text{cholesterol} + \beta_8 \times \text{gluc} + \beta_9 \times \text{smoke} + \beta_{10} \times \text{alco}\\
& + \beta_{11} \times \text{active})
\end{aligned}
}{%
\begin{aligned}[t]
& 1 + \exp(\beta_0 + \beta_1 \times \text{age}+ \beta_2 \times \text{height}  + \beta_3 \times \text{weight} + \beta_4 \times \text{gender} + \beta_5 \times \text{ap\_hi} \\
& + \beta_6 \times \text{ap\_lo} + \beta_7 \times \text{cholesterol} + \beta_8 \times \text{gluc} + \beta_9 \times \text{smoke} + \beta_{10} \times \text{alco}\\
& + \beta_{11} \times \text{active})
\end{aligned}
}
\end{align}

**Model 1 - Include all predictors**

```{r}
library(readr)
cardio_train <- read_delim("cardio_train.csv",
                           delim = ";", escape_double = FALSE, trim_ws = TRUE)

cardio_train$gender = as.factor(cardio_train$gender)
cardio_train$cholesterol = as.factor(cardio_train$cholesterol)
cardio_train$gluc = as.factor(cardio_train$gluc)
cardio_train$smoke = as.factor(cardio_train$smoke)
cardio_train$alco = as.factor(cardio_train$alco)
cardio_train$active = as.factor(cardio_train$active)
cardio_train$cardio = as.factor(cardio_train$cardio)

heart.logistic1 =  glm(cardio  ~  . - id,  family  =  "binomial",
                      data  =  cardio_train)

summary(heart.logistic1)

```

Predictors with p-values less than 0.05 are considered significant. For example, age, height, weight, blood pressure, cholesterol levels, smoking status, alcohol consumption, and physical activity are all significant predictors. Gender, glucose level don't appear to be significant predictors as their p-values exceed chosen significance level.

**Model 2 - Only include statistically significant predictors**


We will use the significant predictors found from heart.logistic1 to to create Model 2. This means all predictors except for id, gender, and glucose will be included.


```{r, warning=FALSE}
heart.logistic2 =  glm(cardio  ~  age+height+weight+ap_hi+ap_lo+cholesterol+smoke+alco+active
                         ,  family  =  "binomial",
                      data  =  cardio_train)

summary(heart.logistic2)
```

```{r,  warning=FALSE}
step(heart.logistic1)
```

**Model 3 - Using predictors from stepwise regression**

The step() function was applied on the initial model (heart.logistic1) to perform stepwise regression based on the Akaike Information Criterion (AIC). The stepwise process sequentially evaluates each predictor's contribution to the model and removes predictors that do not significantly improve the model fit, based on AIC.

After the stepwise regression process, the final selected predictors were age, height, weight, ap_hi (systolic blood pressure), ap_lo (diastolic blood pressure), cholesterol, gluc, smoke, alco (alcohol consumption), and active (physical activity). The coefficients for these predictors are provided in the summary output of the final model (heart.logistic3).

Interpretation of Results: The coefficients in the final model represent the log odds of the outcome (cardiovascular disease) associated with each unit change in the predictor, holding other predictors constant. For example, a positive coefficient for a predictor indicates an increase in the log odds of cardiovascular disease with an increase in that predictor, while a negative coefficient indicates a decrease in the log odds. The significance of each predictor is determined by its corresponding p-value, with predictors having p-values less than the chosen significance level (typically 0.05) considered statistically significant.

```{r,  warning=FALSE}
heart.logistic3 = glm(formula = cardio ~ age + height + weight + ap_hi + ap_lo + 
    cholesterol + gluc + smoke + alco + active, family = "binomial", 
    data = cardio_train)
summary(heart.logistic3)
```

**Determining Best Model**

```{r}
extract_info  = function(model) {
  deviance <- summary(model)$null.deviance
  residual_deviance <- summary(model)$deviance
  r_squared <- 1 - (residual_deviance / deviance)
  AIC <- AIC(model)
  BIC <- BIC(model)
  
  return(c(Null_Deviance = deviance,
           Residual_Deviance = residual_deviance,
           R_Squared = r_squared,
           AIC = AIC,
           BIC = BIC))
}

# Extract information from each model
info1 <- extract_info(heart.logistic1)
info2 <- extract_info(heart.logistic2)
info3 <- extract_info(heart.logistic3)

# Create a data frame to store the information
model_info <- data.frame(
  Model = c("heart.logistic1", "heart.logistic2", "heart.logistic3"),
  Null_Deviance = c(info1["Null_Deviance"], info2["Null_Deviance"], info3["Null_Deviance"]),
  Residual_Deviance = c(info1["Residual_Deviance"], info2["Residual_Deviance"], info3["Residual_Deviance"]),
  R_Squared = c(info1["R_Squared"], info2["R_Squared"], info3["R_Squared"]),
  AIC = c(info1["AIC"], info2["AIC"], info3["AIC"]),
  BIC = c(info1["BIC"], info2["BIC"], info3["BIC"])
)
(model_info)
```

It is evident that \textcolor{red}{Model 3} is the best model out of all the regression models because the AIC and BIC is the lowest. The AIC of Model 3 is 80907.87 while the AIC of Model 1 is 80910.62 and the AIC of Model 2 is 81005.71. We will further test Model 3 (Best Model) on trainign and validation tests.

### Final Equation for Logistic Regression Model

\begin{align}
P(\text{cardio}=1|X) = \frac{%
\begin{aligned}[t]
& \exp(-8.188 + 0.0001485 \times \text{age}+ -0.005255 \times \text{height}  + 0.01521 \times \text{weight} \\
& + 0.03953\times \text{ap\_hi} + 0.0003007\times \text{ap\_lo} + 0.4218 \times \text{cholesterol2} \\
& + 1.134 \times \text{cholesterol3} +  0.03005 \times \text{gluc2} +  -0.3389 \times \text{gluc3} \\
& + -0.1256\times \text{smoke1} + -0.1681 \times \text{alco1} + -0.2101 \times \text{active1})
\end{aligned}
}{%
\begin{aligned}[t]
& 1+ \exp(-8.188 + 0.0001485 \times \text{age}+ -0.005255 \times \text{height}  + 0.01521 \times \text{weight} \\
& + 0.03953\times \text{ap\_hi} + 0.0003007\times \text{ap\_lo} + 0.4218 \times \text{cholesterol2} \\
& + 1.134 \times \text{cholesterol3} +  0.03005 \times \text{gluc2} +  -0.3389 \times \text{gluc3} \\
& + -0.1256\times \text{smoke1} + -0.1681 \times \text{alco1} + -0.2101 \times \text{active1})
\end{aligned}
}
\end{align}

### Training/ Validation

```{r, warning=FALSE}
set.seed(100)
test_errors = numeric(10)
for(i in 1:10){
  # initialize vector to store prediction errors
sample= sample.int(n = nrow(cardio_train), size = floor(0.80*nrow(cardio_train)))
train.heart.logistic = cardio_train[sample,]
test.heart.logistic = cardio_train[-sample,]

train.logistic = glm(formula = cardio ~ age + height + weight + ap_hi + ap_lo + 
    cholesterol + gluc + smoke + alco + active, family = "binomial", 
    data = cardio_train)

glm.pred = predict.glm(train.logistic, newdata = test.heart.logistic, type = "response")

pred = predict(train.logistic, type = "response", newdata = test.heart.logistic)
val = ifelse(pred <0.5,"0", "1")
tab = table(val, test.heart.logistic$cardio)
test_errors[i] = (tab[2]+tab[3])/(tab[1]+tab[2]+tab[3]+tab[4])
}
(mean_test_error = mean(test_errors))


```


The dataset is randomly split into training and validation sets using an 80-20 split. The logistic regression model uses predictors from heart.logistic3 which are age, height, weight, blood pressure (ap_hi and ap_lo), cholesterol level, glucose level, smoking status, alcohol consumption, and physical activity. 

The trained model is then used to predict the probability of cardiovascular disease for the validation set using the predict.glm() function. 

Predicted probabilities are converted to binary predictions using a threshold of 0.5, where probabilities greater than 0.5 are classified as 1 (indicating presence of cardiovascular disease) and probabilities less than or equal to 0.5 are classified as 0 (indicating absence of cardiovascular disease). 

The test error rate is calculated by creating a confusion matrix and dividing the sum of the misclassified observations (false positives and false negatives) by all the observations. Mean Test Error Rate Calculation: The mean error rate is calculated by averaging the test error rates obtained from 10 iterations of the validation process. In each iteration, the logistic regression model is trained on a randomly selected 80% of the data, and the mean error rate is calculated based on predictions made on the remaining 20% of the data. The test error rate provides an estimate of the model's performance in predicting cardiovascular disease risk. 

Mean Error Rate Result: The mean error rate obtained from the validation process is approximately 27.74%. This indicates that, on average, the logistic regression model misclassifies cardiovascular disease status in the validation set for 27.74% of observations. 

### Results

Based on the logistic regression models trained and validated for predicting cardiovascular disease risk, several important insights can be drawn. 

The inclusion of statistically significant predictors, such as age, blood pressure, cholesterol levels, smoking status, alcohol consumption, and physical activity, in the final models underscores their crucial roles in assessing cardiovascular risk. These findings align with established medical knowledge regarding the impact of these factors on heart health. Additionally, the error rate obtained from the validation process indicates that the logistic regression models are performing well in predicting cardiovascular disease status, with an average misclassification rate of only around 27.74%.

Overall, the results suggest that logistic regression modeling, when incorporating significant predictors, can effectively predict cardiovascular disease risk. By leveraging key demographic, clinical, and lifestyle factors, healthcare practitioners can utilize these models to identify individuals at higher risk of cardiovascular disease and tailor preventive strategies and interventions accordingly. These findings contribute to a better understanding of cardiovascular risk assessment and highlight the potential of logistic regression models in supporting proactive heart health management strategies.
